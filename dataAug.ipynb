{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3d2a687-6436-4307-a6bf-e925bb82fd75",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import cv2\n",
    "import os\n",
    "import json\n",
    "import random\n",
    "import numpy as np\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.polys import Polygon, PolygonsOnImage\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from PIL import Image\n",
    "np.sctypes = {\n",
    "    \"float\": [np.float16, np.float32, np.float64],\n",
    "    \"int\": [np.int8, np.int16, np.int32, np.int64],\n",
    "    \"uint\": [np.uint8, np.uint16, np.uint32, np.uint64],\n",
    "    \"complex\": [np.complex64, np.complex128]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0de007-f73e-42d2-bce0-3f6b3f971ad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import sys\n",
    "#sys.path.append('/Users/peiyu.li/CS231N')\n",
    "\n",
    "apklot_training_path = '/Users/joshneutel/code/APKLOT/1. Satellite/Dataset/World/training'\n",
    "apklot_testing_path = '/Users/joshneutel/code/APKLOT/1. Satellite/Dataset/World/testing'\n",
    "\n",
    "train_output_path = '/Users/joshneutel/Desktop/APKLOT/training'\n",
    "test_output_path = '/Users/joshneutel/Desktop/APKLOT/testing'\n",
    "\n",
    "aug_output_path = '/Users/joshneutel/Desktop/augmented'\n",
    "final_aug_output_path = '/Users/joshneutel/Desktop/final_augmented'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a81982c0-38ab-468f-8b3a-5f299ed8a0cc",
   "metadata": {},
   "source": [
    "## Visualize APKLOT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0617c1c-99d3-4eac-80a6-0f7717a4251a",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_FILES = os.listdir(apklot_training_path)\n",
    "os.makedirs(aug_output_path, exist_ok=True)\n",
    "RAW_FILES = [\n",
    "    file.replace(\".json\", \"\").replace(\".png\", \"\")\n",
    "    for file in RAW_FILES\n",
    "]\n",
    "RAW_FILES = list(set(RAW_FILES))  # Remove duplicates\n",
    "print(f\"Total number of RAW_FILES: {len(RAW_FILES)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9cff570-8839-4c1c-a2e2-e333b79f2180",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(image_str, with_annotation=True):\n",
    "    # Load image\n",
    "    image = Image.open(f\"{apklot_training_path}/{image_str}.png\")\n",
    "\n",
    "    # Load JSON annotations\n",
    "    with open(f\"{apklot_training_path}/{image_str}.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Plot image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if with_annotation:\n",
    "        # Plot polygons\n",
    "        for shape in data['shapes']:\n",
    "            points = shape['points']\n",
    "            polygon = patches.Polygon(points, closed=True, edgecolor='red', facecolor='red', linewidth=2, alpha=0.5)\n",
    "            ax.add_patch(polygon)\n",
    "\n",
    "    plt.axis('off')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4f7652-1b26-4956-a451-c10254ba622b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = RAW_FILES[0]\n",
    "plt = load_image(file, with_annotation=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c3755bf-5d44-4abd-bcc8-9d907b3887dc",
   "metadata": {},
   "source": [
    "# Data Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a211720-3d4e-4f34-8c2b-891a780c7c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "RANDOM_SEED = 42\n",
    "random.seed(RANDOM_SEED)\n",
    "np.random.seed(RANDOM_SEED)\n",
    "ia.seed(RANDOM_SEED)\n",
    "\n",
    "# Number of augmented versions to create per original image\n",
    "SAMPLES_PER_IMAGE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16608493-92ee-48a6-8568-cd874f808784",
   "metadata": {},
   "source": [
    "## Helper functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1d7f32-ead5-461e-86c5-dadfb77371f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the data augmentation sequence for jittering\n",
    "def create_aug_seq():\n",
    "    \"\"\"Create an imgaug augmentation sequence for jittering\"\"\"\n",
    "    return iaa.Sequential([\n",
    "        iaa.Crop(px=(0, 50)),  # Random crop between 0-50 pixels\n",
    "        iaa.Fliplr(0.5),       # Horizontal flip with 50% probability\n",
    "        iaa.Flipud(0.5),       # Vertical flip with 50% probability\n",
    "        iaa.Affine(rotate=(-45, 45))  # Random rotation between -45 and 45 degrees\n",
    "    ])\n",
    "\n",
    "def load_image_and_json(image_path, json_path):\n",
    "    \"\"\"Load image and its JSON annotation\"\"\"\n",
    "    image = Image.open(image_path)\n",
    "    image_array = np.array(image)\n",
    "\n",
    "    with open(json_path, 'r') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    return image_array, json_data\n",
    "\n",
    "def extract_polygons(json_data):\n",
    "    \"\"\"Extract polygons from JSON data in imgaug format\"\"\"\n",
    "    polygons = []\n",
    "\n",
    "    for shape in json_data['shapes']:\n",
    "        points = shape['points']\n",
    "        # Convert points to imgaug polygon format\n",
    "        polygon = Polygon(points)\n",
    "        polygons.append(polygon)\n",
    "\n",
    "    return polygons\n",
    "\n",
    "def apply_augmentation(image, polygons, seq):\n",
    "    \"\"\"Apply augmentation to image and polygons\"\"\"\n",
    "    # Create PolygonsOnImage object\n",
    "    polys_on_image = PolygonsOnImage(polygons, shape=image.shape)\n",
    "\n",
    "    # Apply augmentation\n",
    "    image_aug, polys_aug = seq(image=image, polygons=polys_on_image)\n",
    "\n",
    "    return image_aug, polys_aug\n",
    "\n",
    "def update_json_with_augmented_polygons(json_data, polys_aug):\n",
    "    \"\"\"Update JSON data with augmented polygons\"\"\"\n",
    "    aug_json = copy.deepcopy(json_data)\n",
    "\n",
    "    for i, shape in enumerate(aug_json['shapes']):\n",
    "        if i < len(polys_aug):\n",
    "            shape['points'] = polys_aug[i].exterior.tolist()\n",
    "\n",
    "    return aug_json\n",
    "\n",
    "def save_augmented_image_and_json(image_aug, json_aug, output_img_path, output_json_path):\n",
    "    \"\"\"Save augmented image and JSON data\"\"\"\n",
    "    # Save image\n",
    "    Image.fromarray(image_aug).save(output_img_path)\n",
    "\n",
    "    # Save JSON\n",
    "    with open(output_json_path, 'w') as f:\n",
    "        json.dump(json_aug, f, indent=2)\n",
    "\n",
    "# Define function to load and visualize images with annotations\n",
    "def load_image(image_str, path, with_annotation=True):\n",
    "    # Load image\n",
    "    image = Image.open(f\"{path}/{image_str}.png\")\n",
    "\n",
    "    # Load JSON annotations\n",
    "    with open(f\"{path}/{image_str}.json\") as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Plot image\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.imshow(image)\n",
    "\n",
    "    if with_annotation:\n",
    "        # Plot polygons\n",
    "        for shape in data['shapes']:\n",
    "            points = shape['points']\n",
    "            polygon = patches.Polygon(points, closed=True, edgecolor='red', facecolor='red', linewidth=2, alpha=0.5)\n",
    "            ax.add_patch(polygon)\n",
    "\n",
    "    plt.axis('off')\n",
    "    return plt\n",
    "\n",
    "# Function to visualize original and augmented images\n",
    "def visualize_comparison(original_img, original_json, augmented_img, augmented_json, title=\"Original vs Augmented\"):\n",
    "    \"\"\"Visualize original and augmented images with their annotations\"\"\"\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "    # Original image\n",
    "    ax1.imshow(original_img)\n",
    "    for shape in original_json['shapes']:\n",
    "        points = shape['points']\n",
    "        polygon = patches.Polygon(points, closed=True, edgecolor='red', facecolor='red', linewidth=2, alpha=0.5)\n",
    "        ax1.add_patch(polygon)\n",
    "    ax1.set_title(\"Original\")\n",
    "    ax1.axis('off')\n",
    "\n",
    "    # Augmented image\n",
    "    ax2.imshow(augmented_img)\n",
    "    for shape in augmented_json['shapes']:\n",
    "        points = shape['points']\n",
    "        polygon = patches.Polygon(points, closed=True, edgecolor='red', facecolor='red', linewidth=2, alpha=0.5)\n",
    "        ax2.add_patch(polygon)\n",
    "    ax2.set_title(\"Augmented\")\n",
    "    ax2.axis('off')\n",
    "\n",
    "    plt.suptitle(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Main function to perform data augmentation\n",
    "def augment_training_dataset(input_path, aug_output_path, samples_per_image=100):\n",
    "    \"\"\"Augment the training dataset with jittering transformations\"\"\"\n",
    "    # Get list of all training files\n",
    "    train_files = [f.replace('.json', '') for f in os.listdir(input_path)\n",
    "                   if f.endswith('.json')]\n",
    "\n",
    "    print(f\"Found {len(train_files)} original training images\")\n",
    "    print(f\"Generating {samples_per_image} augmented versions for each\")\n",
    "    print(f\"Target total: {len(train_files) * (samples_per_image)} augmented images\")\n",
    "\n",
    "    # Copy original files to output directory\n",
    "    for file_id in train_files:\n",
    "        if not os.path.exists(os.path.join(aug_output_path, f\"{file_id}.png\")):\n",
    "            # Copy original image\n",
    "            shutil.copy(\n",
    "                os.path.join(input_path, f\"{file_id}.png\"),\n",
    "                os.path.join(aug_output_path, f\"{file_id}.png\")\n",
    "            )\n",
    "\n",
    "        if not os.path.exists(os.path.join(aug_output_path, f\"{file_id}.json\")):\n",
    "            # Copy original annotation\n",
    "            shutil.copy(\n",
    "                os.path.join(input_path, f\"{file_id}.json\"),\n",
    "                os.path.join(aug_output_path, f\"{file_id}.json\")\n",
    "            )\n",
    "\n",
    "    # Create a list to store successful augmentations\n",
    "    augmented_files = []\n",
    "\n",
    "    # Process each training image\n",
    "    for i, file_id in enumerate(tqdm(train_files)):\n",
    "        img_path = os.path.join(input_path, f\"{file_id}.png\")\n",
    "        json_path = os.path.join(input_path, f\"{file_id}.json\")\n",
    "\n",
    "        if not (os.path.exists(img_path) and os.path.exists(json_path)):\n",
    "            print(f\"Warning: Missing files for {file_id}, skipping...\")\n",
    "            continue\n",
    "\n",
    "        # Load image and annotation\n",
    "        image, json_data = load_image_and_json(img_path, json_path)\n",
    "\n",
    "        # Extract polygons\n",
    "        polygons = extract_polygons(json_data)\n",
    "\n",
    "        # Create augmented versions\n",
    "        for j in range(samples_per_image):\n",
    "            try:\n",
    "                # Create a new augmentation sequence for each sample\n",
    "                seq = create_aug_seq().to_deterministic()\n",
    "\n",
    "                # Apply augmentation\n",
    "                image_aug, polys_aug = apply_augmentation(image, polygons, seq)\n",
    "\n",
    "                # Update JSON with augmented polygons\n",
    "                json_aug = update_json_with_augmented_polygons(json_data, polys_aug)\n",
    "\n",
    "                # Generate augmented file ID\n",
    "                aug_id = f\"{file_id}_aug_{j+1}\"\n",
    "\n",
    "                # Save augmented image and JSON\n",
    "                aug_img_path = os.path.join(aug_output_path, f\"{aug_id}.png\")\n",
    "                aug_json_path = os.path.join(aug_output_path, f\"{aug_id}.json\")\n",
    "\n",
    "                save_augmented_image_and_json(image_aug, json_aug, aug_img_path, aug_json_path)\n",
    "\n",
    "                augmented_files.append(aug_id)\n",
    "\n",
    "                # Visualize the first augmentation of the first few images\n",
    "                if i < 3 and j == 0:\n",
    "                    visualize_comparison(image, json_data, image_aug, json_aug,\n",
    "                                         title=f\"Original vs Augmented: {file_id}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error augmenting {file_id} (sample {j+1}): {e}\")\n",
    "\n",
    "    print(f\"Data augmentation complete!\")\n",
    "    print(f\"Created {len(augmented_files)} augmented images\")\n",
    "    print(f\"Total images in output directory: {len(train_files) + len(augmented_files)}\")\n",
    "\n",
    "    return augmented_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d547663-4550-4337-9ce4-7de548200587",
   "metadata": {},
   "source": [
    "## Augment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3e3c05a-c162-41c5-b914-10bd393e5b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import shutil\n",
    "\n",
    "# # Now let's augment the training dataset\n",
    "augmented_files = augment_training_dataset(apklot_training_path, aug_output_path, SAMPLES_PER_IMAGE)\n",
    "\n",
    "# # Count files in output directory to verify\n",
    "output_files = [f.replace('.png', '') for f in os.listdir(aug_output_path) if f.endswith('.png')]\n",
    "print(f\"Total files in output directory: {len(output_files)}\")\n",
    "print(f\"Original training images: {len([f for f in output_files if not '_aug_' in f])}\")\n",
    "print(f\"Augmented training images: {len([f for f in output_files if '_aug_' in f])}\")\n",
    "\n",
    "# Visualize a few examples\n",
    "print(\"\\nVisualization examples:\")\n",
    "if len(output_files) > 0:\n",
    "    # Show an original image\n",
    "    original_sample = next((f for f in output_files if not '_aug_' in f), None)\n",
    "    if original_sample:\n",
    "        print(\"Original sample:\")\n",
    "        plt = load_image(original_sample, aug_output_path)\n",
    "        plt.show()\n",
    "    # Show an augmented image\n",
    "    augmented_sample = next((f for f in output_files if '_aug_' in f), None)\n",
    "    if augmented_sample:\n",
    "        print(\"Augmented sample:\")\n",
    "        plt = load_image(augmented_sample, aug_output_path)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a6ff1e-96d1-4ce4-8e7e-b8273588f13b",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_train_actual = len([f for f in os.listdir(aug_output_path)])\n",
    "print(f\"Number of training data: {n_train_actual}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8ea92f-4b52-4f43-a6fe-9b8852bd63e9",
   "metadata": {},
   "source": [
    "# Convert data to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8617339f-3381-4d8e-bf17-e8495afec054",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_poly_to_mask(polygons, image_shape):\n",
    "    mask = np.zeros(image_shape, dtype=np.uint8)\n",
    "    for polygon in polygons:\n",
    "        coords = np.array(polygon.exterior, dtype=np.int32)\n",
    "        coords = coords.reshape((-1, 1, 2))  # Required shape for cv2.fillPoly\n",
    "        cv2.fillPoly(mask, [coords], 1)\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f6a6ca-9939-46a1-8800-345f43409227",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_overlay(image_path, mask_path, alpha=0.4, mask_color=(255, 0, 0)):\n",
    "    # Load image and mask\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    mask = Image.open(mask_path).convert(\"L\")  # grayscale\n",
    "\n",
    "    # Convert to NumPy\n",
    "    image_np = np.array(image)\n",
    "    mask_np = np.array(mask)\n",
    "\n",
    "    # Create a color mask\n",
    "    color_mask = np.zeros_like(image_np)\n",
    "    color_mask[mask_np > 0] = mask_color\n",
    "\n",
    "    # Overlay\n",
    "    overlay = image_np.copy()\n",
    "    overlay = np.where(mask_np[..., None] > 0,\n",
    "                       (1 - alpha) * image_np + alpha * color_mask,\n",
    "                       image_np).astype(np.uint8)\n",
    "\n",
    "    # Display\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(overlay)\n",
    "    plt.axis(\"off\")\n",
    "    plt.title(\"Image with Mask Overlay\")\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b60d452f-0bd8-4e5a-aaed-4158926c5bb5",
   "metadata": {},
   "source": [
    "## APKLOT training to png "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4439a3e7-5af7-4a36-8667-5953fce1ef60",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FILES = os.listdir(apklot_training_path)\n",
    "TRAIN_FILES = [\n",
    "    file.replace(\".json\", \"\").replace(\".png\", \"\")\n",
    "    for file in TRAIN_FILES\n",
    "]\n",
    "TRAIN_FILES = list(set(TRAIN_FILES))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a93ceafd-8ad3-43b5-8745-9b3edf599a5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(TRAIN_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52894f74-c98c-4c75-83b5-20e67817e64c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(TRAIN_FILES)):\n",
    "    file_id = TRAIN_FILES[i]\n",
    "    try:\n",
    "        image = Image.open(f\"{apklot_training_path}/{file_id}.png\")\n",
    "    except:\n",
    "        print(f\"Issue with {file_id}\")\n",
    "        continue\n",
    "\n",
    "    # Load JSON annotations\n",
    "    try:\n",
    "        with open(f\"{apklot_training_path}/{file_id}.json\") as f:\n",
    "            json_data = json.load(f)\n",
    "    except:\n",
    "        print(f\"Issue with {file_id}\")\n",
    "        continue\n",
    "    \n",
    "    polygons = extract_polygons(json_data)\n",
    "    width, height = image.size\n",
    "    image_shape = (height, width)\n",
    "    mask_array = convert_poly_to_mask(polygons, image_shape)\n",
    "    mask_image = Image.fromarray(mask_array).convert(\"L\")\n",
    "    \n",
    "\n",
    "    image.save(f\"{train_output_path}/images/{i}.png\")\n",
    "    mask_image.save(f\"{train_output_path}/masks/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c7ed71-f1e7-4919-98e0-fc696d75848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = visualize_overlay(\n",
    "    f\"{train_output_path}/images/{370}.png\", \n",
    "    f\"{train_output_path}/masks/{370}.png\",\n",
    "    alpha=0.7, mask_color=(0, 0, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01093647-ac99-4c4f-89b6-eb22d5a9dcc8",
   "metadata": {},
   "source": [
    "## APKLOT testing to png "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff001be-2d5e-4400-a1f8-9a5f65801a26",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FILES = os.listdir(apklot_testing_path)\n",
    "TEST_FILES = [\n",
    "    file.replace(\".json\", \"\").replace(\".png\", \"\")\n",
    "    for file in TEST_FILES\n",
    "]\n",
    "TEST_FILES = list(set(TEST_FILES))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2eb857e-3dab-4599-8af2-1fe988d79232",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(TEST_FILES)):\n",
    "    file_id = TEST_FILES[i]\n",
    "    try:\n",
    "        image = Image.open(f\"{apklot_testing_path}/{file_id}.png\")\n",
    "    except:\n",
    "        print(f\"Issue with {file_id}\")\n",
    "        continue\n",
    "\n",
    "    # Load JSON annotations\n",
    "    try:\n",
    "        with open(f\"{apklot_testing_path}/{file_id}.json\") as f:\n",
    "            json_data = json.load(f)\n",
    "    except:\n",
    "        print(f\"Issue with {file_id}\")\n",
    "        continue\n",
    "    \n",
    "    polygons = extract_polygons(json_data)\n",
    "    width, height = image.size\n",
    "    image_shape = (height, width)\n",
    "    mask_array = convert_poly_to_mask(polygons, image_shape)\n",
    "    mask_image = Image.fromarray(mask_array).convert(\"L\")\n",
    "    \n",
    "\n",
    "    image.save(f\"{test_output_path}/images/{i}.png\")\n",
    "    mask_image.save(f\"{test_output_path}/masks/{i}.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "547109c0-faf9-4f88-8edf-ea20b3ab0b5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt = visualize_overlay(\n",
    "    f\"{test_output_path}/images/{80}.png\", \n",
    "    f\"{test_output_path}/masks/{80}.png\",\n",
    "    alpha=0.7, mask_color=(0, 0, 0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406056f2-7e6f-4d9e-8c1d-20485b6949ee",
   "metadata": {},
   "source": [
    "## Augmentation to png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8af968f3-c75e-4e62-864f-81ac2685739c",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUG_FILES = os.listdir(aug_output_path)\n",
    "AUG_FILES = [\n",
    "    file.replace(\".json\", \"\").replace(\".png\", \"\")\n",
    "    for file in AUG_FILES\n",
    "]\n",
    "AUG_FILES = list(set(AUG_FILES))  # Remove duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1baa81-0c21-4f7d-8c56-f036ad46ac25",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(AUG_FILES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4bbd40e-2033-4804-a65b-01724aae3fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "END_NUM = 27488 + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef70a46-e845-4d09-9484-10f3d6fb65cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(AUG_FILES)):\n",
    "    file_id = AUG_FILES[i]\n",
    "    try:\n",
    "        image = Image.open(f\"{aug_output_path}/{file_id}.png\")\n",
    "    except:\n",
    "        print(f\"Issue with {file_id}\")\n",
    "        continue\n",
    "\n",
    "    # Load JSON annotations\n",
    "    try:\n",
    "        with open(f\"{aug_output_path}/{file_id}.json\") as f:\n",
    "            json_data = json.load(f)\n",
    "    except:\n",
    "        print(f\"Issue with {file_id}\")\n",
    "        continue\n",
    "    \n",
    "    polygons = extract_polygons(json_data)\n",
    "    width, height = image.size\n",
    "    image_shape = (height, width)\n",
    "    mask_array = convert_poly_to_mask(polygons, image_shape)\n",
    "    mask_image = Image.fromarray(mask_array).convert(\"L\")\n",
    "    \n",
    "\n",
    "    image.save(f\"{final_aug_output_path}/images/{i + END_NUM}.png\")\n",
    "    mask_image.save(f\"{final_aug_output_path}/masks/{i + END_NUM}.png\")\n",
    "    \n",
    "    os.remove(f\"{aug_output_path}/{file_id}.png\")\n",
    "    os.remove(f\"{aug_output_path}/{file_id}.json\")\n",
    "        \n",
    "    if (i % 1000 == 0):\n",
    "        print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639ffaa7-10b9-4626-b6a0-95594a30c794",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt = visualize_overlay(\n",
    "#    f\"{final_aug_output_path}/images/{10000}.png\", \n",
    "#    f\"{final_aug_output_path}/masks/{10000}.png\",\n",
    "#    alpha=0.7, mask_color=(0, 0, 0)\n",
    "#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd1d983-0014-4f01-9496-0d532bb57206",
   "metadata": {},
   "outputs": [],
   "source": [
    "FINAL_FILES = os.listdir(f\"{final_aug_output_path}/images/\")\n",
    "FINAL_FILES.remove('.DS_Store')\n",
    "FINAL_FILES = [\n",
    "    int(file.replace(\".json\", \"\").replace(\".png\", \"\"))\n",
    "    for file in FINAL_FILES\n",
    "]\n",
    "FINAL_FILES = list(set(FINAL_FILES))  # Remove duplicates\n",
    "FINAL_FILES.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfaf95de-9034-4b95-a4ac-be71c1bc91e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_path = os.path.join(final_aug_output_path, \"metadata.jsonl\")\n",
    "with open(metadata_path, \"w\") as metadata_file:\n",
    "    for file in FINAL_FILES:\n",
    "        # Write entry to JSONL\n",
    "        metadata_entry = {\"image\": f\"images/{file}.png\", \"mask\": f\"masks/{file}.png\"}\n",
    "        metadata_file.write(json.dumps(metadata_entry) + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe1c64d-9947-4411-bf96-ad0b3d2622bb",
   "metadata": {},
   "source": [
    "## Move to chunks of 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b945c698-2b08-403a-b7da-27f2ace86fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_dir = os.path.join(final_aug_output_path, \"images\")\n",
    "masks_dir = os.path.join(final_aug_output_path, \"masks\")\n",
    "output_metadata_path = os.path.join(final_aug_output_path, \"metadata_sharded.jsonl\")\n",
    "shard_size = 8000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efb2d31-a7f7-4ae8-93fb-037e8523e1ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shard_path(file_index, folder_base):\n",
    "    shard_id = (file_index // shard_size) * shard_size\n",
    "    subfolder = os.path.join(folder_base, f\"{shard_id:05d}\")\n",
    "    return subfolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d4530b5-9c7a-4dd9-8c83-bf571ae2d085",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make subfolders and move files\n",
    "with open(output_metadata_path, \"w\") as outfile:\n",
    "    for i, filename in enumerate(sorted(os.listdir(images_dir))):\n",
    "        if not filename.endswith(\".png\"):\n",
    "            continue\n",
    "        base = os.path.splitext(filename)[0]\n",
    "\n",
    "        # Compute shard paths\n",
    "        image_subdir = shard_path(int(base), images_dir)\n",
    "        mask_subdir = shard_path(int(base), masks_dir)\n",
    "\n",
    "        os.makedirs(image_subdir, exist_ok=True)\n",
    "        os.makedirs(mask_subdir, exist_ok=True)\n",
    "\n",
    "        # Move files\n",
    "        src_image = os.path.join(images_dir, filename)\n",
    "        dst_image = os.path.join(image_subdir, filename)\n",
    "        shutil.move(src_image, dst_image)\n",
    "\n",
    "        src_mask = os.path.join(masks_dir, filename)\n",
    "        dst_mask = os.path.join(mask_subdir, filename)\n",
    "        shutil.move(src_mask, dst_mask)\n",
    "\n",
    "        # Write updated metadata line\n",
    "        relative_image_path = os.path.relpath(dst_image, final_aug_output_path)\n",
    "        relative_mask_path = os.path.relpath(dst_mask, final_aug_output_path)\n",
    "        metadata_entry = {\"image\": relative_image_path, \"mask\": relative_mask_path}\n",
    "        outfile.write(json.dumps(metadata_entry) + \"\\n\")\n",
    "\n",
    "        if i % 1000 == 0:\n",
    "            print(f\"Processed {i} files...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154a1f7-b547-4f2d-b3f5-4f4daa6ca534",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unshard_dataset(root_dir):\n",
    "    images_dir = os.path.join(root_dir, \"images\")\n",
    "    masks_dir = os.path.join(root_dir, \"masks\")\n",
    "    print(\"Unsharding images...\")\n",
    "    for subdir in sorted(os.listdir(images_dir)):\n",
    "        subdir_path = os.path.join(images_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for file in os.listdir(subdir_path):\n",
    "                if file.endswith(\".png\"):\n",
    "                    src = os.path.join(subdir_path, file)\n",
    "                    dst = os.path.join(images_dir, file)\n",
    "                    shutil.move(src, dst)\n",
    "            os.rmdir(subdir_path)  # remove empty subfolder\n",
    "\n",
    "    print(\"Unsharding masks...\")\n",
    "    for subdir in sorted(os.listdir(masks_dir)):\n",
    "        subdir_path = os.path.join(masks_dir, subdir)\n",
    "        if os.path.isdir(subdir_path):\n",
    "            for file in os.listdir(subdir_path):\n",
    "                if file.endswith(\".png\"):\n",
    "                    src = os.path.join(subdir_path, file)\n",
    "                    dst = os.path.join(masks_dir, file)\n",
    "                    shutil.move(src, dst)\n",
    "            os.rmdir(subdir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b69502-f4cb-47e0-b40e-a7a816825517",
   "metadata": {},
   "outputs": [],
   "source": [
    "unshard_dataset(final_aug_output_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (base)",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
